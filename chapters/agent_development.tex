\chapter{Agent Development}
\label{cha:agent_development}

In this chapter we present the iterative development process of the agent. We describe
the successive phases of its evolution, from the initial prototype to the final
implementation. During this journey, several challenges and unexpected issues
emerged. For each phase, we detail the encountered problems and the approaches
adopted to overcome them, explaining the reasoning behind the design choices.
The majority of the choices taken in the crafting of the various prompt will be
discussed in Chapter \ref{cha:data_collection}, while reporting here all the discarded
ones.

\section{First Approach}
\label{sec:first_approach}

In this initial phase of the development and testing process, a trial-and-error methodology
was adopted to iteratively refine the system's behavior and try to optimize performance.
Unfortunately, this led to moving away from the definition of the problem and,
in combination with the poor performance of the agent, it was decided to start over
with a new approach. Nonetheless, this first attempt was crucial in understanding
the challenges and limitations of the problem, so it is important to describe it.

\vspace{1mm}
\begin{codewindow}
  [Text] \lstset{style=pythonstyle, caption={Scheme of a prompt used in the first approach, full in Appendix \ref{lst:apx:first_agent_prompt}},
  label={lst:first_agent_prompt}} \begin{lstlisting}
[ROLE DESCRIPTION]

[MAP]

[LEGEND]

[ACTIONS]

[PARCELS ALREADY PICKED]

[RULES]

[QUESTION]
\end{lstlisting}
\end{codewindow}
\vspace{1mm}

The approach began by parsing crucial information from the server, which served
as the foundation for understanding how the LLM would interact with it. The main
point of discuss in the parsing topic was the map, that was represented as a
multi-line string like the one in Listing \ref{lst:parced_map}. The first prompt
sent to the LLM was crafted by concatenating the map with all the other information
needed to describe the state of the environment, as shown in Listing
\ref{lst:first_agent_prompt}.

\vspace{1mm}
\begin{codewindow}
  [Text] \lstset{style=pythonstyle, caption={Parsed Map Result with partial legend},
  label={lst:parced_map}} \begin{lstlisting}
    1 1 1 1 1
    1 1 P 1 1
    1 1 1 1 1
    2 A 1 1 1
    1 1 1 1 1

    LEGEND:
    1: Walkable cell
    2: Delivery point
    A: Agent
    P: Parcel
\end{lstlisting}
\end{codewindow}
\vspace{1mm}

This implementation started as a full raw approach, letting the LLM also decide
the goal to pursue. As various challenges and inefficiencies were identified during
extensive testing, we progressively implemented a total of seven ``helping" parameters.

\subsection{Helping Parameters}
These parameters were introduced with the objective of addressing specific issues
observed during experimentation, and each of them played a significant role in
shaping the overall functionality of the agent:
\begin{itemize}
  \item \texttt{ANTI\_LOOP}: This parameter was introduced to eliminate a common
    inefficiency in agent movement, wherein the agent would repeatedly traverse
    the same path in a circular loop, failing to make meaningful progress toward
    its goal. By setting this parameter to \texttt{true}, if the last four
    action were \texttt{["U", "R", "D", "L"]} (either clockwise or
    counterclockwise) the agent was forced to take an action that prevented the loop
    for happening. This optimization helped the agent make more intelligent
    movement decisions, thereby removing the possibility of being stuck in repetitive
    cycles;

  \item \texttt{HELP\_THE\_BOT}: The primary purpose of this parameter was to assist
    the agent in handling parcels more effectively. When activated by setting it
    to \texttt{true}, the agent was programmed to automatically take a parcel if
    the parcel was located directly below its current position. Additionally, if
    the agent was positioned at a delivery point, this parameter ensured that the
    agent would immediately proceed with shipping the parcel without requiring additional
    decision-making steps. This was implemented to reduce the number of calls to
    the LLM, since, even in this version of the agent, it was able to always pick
    up a parcel and deliver it (if in the correct tile);

  \item \texttt{SELECT\_ONLY\_ACTION}: This parameter was designed to simplify the
    agent's decision-making process in cases where the list of available actions
    contained only a single option. When set to \texttt{true}, the agent would
    automatically select and execute the sole available action without
    hesitation or delay. This was made by a big filtering phase that returned the
    legal actions:
    \begin{itemize}
      \item remove the opposite of the last action;

      \item remove all the action that was not possible (like going left while
        in the first column or going up while in the first row);

      \item remove the delivery action if the agent wasn't carrying a parcel and
        in a delivery point;

      \item remove the pick action if the agent was in a cell with no parcel.
    \end{itemize}
    This, in combination with the \texttt{HELP\_THE\_BOT} parameter, reduced the
    number of unnecessary calls to the LLM, thereby enhancing the agent's
    efficiency, but also giving the agent too little decision power;

  \item \texttt{USE\_HISTORY}: This parameter is the only one that was kept for all
    the future iterations (more on this in Section \ref{sec:stateful}). The role
    of this parameter was to decide whether each call to the LLM should contain
    only the current state of the environment of the entire message history. If set
    to \texttt{true}, the LLM would have access to the full conversation history,
    allowing it to make more informed decisions based on past interactions and
    events. This feature was particularly useful and powerful, but also with a
    big downside related to the LLM context length, that will be discussed in Chapter
    \ref{cha:conclusions};

  \item \texttt{REDUCED\_MAP}: This parameter was introduced to optimize the space
    the map occupied in the prompt by limiting the environment described as a slice
    of the full map and then scaling all the coordinates (of the agent and the
    parcels) treating the reduced map as the total map. The reduction in size
    was determined based on the maximum value between \texttt{PARCELS\_OBSERVATION\_DISTANCE}
    and \texttt{AGENTS\_OBSERVATION\_DISTANCE} (from Server Configuration File,
    see Section \ref{sub:server_configuration_event_handling}), ensuring that
    the agent only received the most relevant spatial data necessary for making
    informed decisions. Essentially, this optimization allowed the attention of
    the LLM to not be too sparse, but bringing some extra problems, for example
    by removing any delivery zone from the map since it was too far away while
    the current goal was to deliver a parcel;

  \item \texttt{HELP\_FIND\_DELIVERY}: This parameter was specifically designed to
    assist the agent in locating delivery points more effectively. By setting it
    to \texttt{true}, the system ensured that the closest delivery point (using Manhattan
    Distance) was always included in the agent's prompt (not as coordinates but as
    directions, eg. "right and up"), even if that particular delivery point was not
    within the agent's immediate field of view. In fact, this parameter was implemented
    to remedy the problem described in the \texttt{REDUCED\_MAP} point. This feature
    provided the agent with valuable directional guidance, allowing it to make
    better routing decisions and reducing the risk of wandering aimlessly in
    search of a delivery location (or worse, by looping again and again), but
    also reduced our ability to track the LLM ability in finding the delivery
    point by itself (more on this in Section \ref{sec:prompts});

  \item \texttt{HELP\_SIMULATE\_NEXT\_ACTIONS}: The goal of this parameter was to
    enhance the agent's decision-making process by simulating and displaying the
    expected outcomes of each possible action. When activated by setting it to
    \texttt{true}, the prompt provided to the LLM included a detailed breakdown
    of how each available action would alter the surrounding environment, by computing
    for each action the resulting map and attaching all of them to the final prompt.
    In theory, this additional information could help the agent anticipate and select
    the most favorable course of action. However, experimental results indicated
    that enabling this feature led to suboptimal performance, resulting in poor
    decision-making and inefficiencies, probably due to the size of the prompt that
    was too big for the LLM to handle.
\end{itemize}

This design resulted in an implementation that was bringing the project in the
wrong direction, because the whole ``no framework on top" idea was breaking down
even if this didn't have anything to do with planning in a strict sense. Without
those helps, the agent was not able to perform well in any environment, and the decision
was made to start over with a new approach.

Overall, while this initial approach did not yield the desired performance, it was
an essential step for the next iterations of the agent's development.

The code for this first implementation can be found in the \texttt{archive/raw\_llm\_agent.js}
file inside the project repository \cite{projectrepo}.

\subsection{Takeaways}
Through this initial approach, we gained valuable insights into the challenges of
designing an effective agent. The key lessons learned from this phase can be
summarized as follows:

Why did this approach not work as expected?
\begin{itemize}
  \item Unclear prompt style: the way information was structured in the prompt
    was not optimal. This became particularly evident in the uncertainty computation,
    where the agent frequently exhibited high uncertainty in its actions;

  \item Over-reliance on helping parameter: providing excessive hints and
    structured input to the agent hindered its ability to explore the
    environment effectively. While guidance could be helpful, too much
    assistance made the results too distant from what the LLM could achieve by
    itself.
\end{itemize}

Key Takeaways from this phase:
\begin{itemize}
  \item Performance: when extensive guidance was provided, the results were still
    acceptable but inferior to those obtained using PDDL-based solutions.
    Initially, we considered implementing a PDDL version of the agent to serve as
    a benchmark. However, as discussed in Section \ref{sub:pddl_based_solutions},
    this comparison would have been unfair due to fundamental differences in
    approach and assumptions;

  \item Unintended biases in LLM behavior: Although not directly related to the
    core functionality of the agent, an interesting observation emerged regarding
    how the LLM interpreted the presence of other agents. The server allowed the
    spawning of ``enemy" agents capable of blocking paths. While this feature
    was not used in the main experiments, we discovered that simply including information
    about these agents in the prompt led the LLM to assume that agents near parcels
    were actively trying to steal them. In reality, these agents were merely obstacles
    with no intent to compete for parcels. This behavior highlights inherent
    biases in the LLM's training data, where similar context might have been associated
    with adversarial interactions.
\end{itemize}

\section{Second Approach}
\label{sec:second_approach}

\subsection{Takeaways}

Possible answers to ``Why didn't this work?" are:
\begin{itemize}
  \item
\end{itemize}

What we brought home:
\begin{itemize}
  \item
\end{itemize}

\section{Final Agent}
\label{sec:final_agent}

\section{Closest Cell to the Goal}
\label{sec:closest_cell_to_the_goal}