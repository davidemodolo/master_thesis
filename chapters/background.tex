\chapter{Background}
\label{cha:background}

In this thesis, we will analyze in detail the behavior of an LLM within a controlled
environment.

Before presenting all the work carried out in detail, this chapter aims to
provide a comprehensive explanation of all the theoretical foundations necessary
to understand the steps presented in the following chapters. In particular, we
want to highlight what an LLM is and how it works, with a special focus on the Attention
mechanism and how the uncertainty of an LLM can be calculated. This will serve
as a basis for correctly interpreting the results analyzed in Section
\ref{cha:results_discussion}.

There will also be a broader discussion on agents in a strict sense and "LLM agents"
to better show the difference between our implementation and what is currently
being discussed over the media.

To better define the context, we will examine the main alternative approaches to
solving a logistical problem currently studied in the literature.

\section{Large Language Models - LLMs}
\label{sec:large_language_models_llms}
\subsection{LLMs' Uncertainty}
\label{sub:llms_uncertainty}

\section{Agents}
\label{sec:agents}
\subsection{BDI Architecture}
\label{sub:bdi_architecture}
\section{State of the Art}
\label{sec:state_of_the_art}